---
title: "An introduction to Machine Learning: Overview"
subtitle: "Data Science option of BIO00058M Data Analysis."
author: "Emma Rand"
institute: "University of York, UK"
output:
  xaringan::moon_reader:
    css: [default, ../css_files/emma.css, ../css_files/emma-fonts.css]
    lib_dir: libs
    seal: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
    fig_caption: false
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE,	
                      warning = FALSE,
                      fig.width = 4, 
                      fig.height = 4, 
                      fig.retina = 3)
options(htmltools.dir.version = FALSE)
```

```{r style-share-again, echo=FALSE}
xaringanExtra::use_share_again()
xaringanExtra::style_share_again(
  share_buttons = "all")
xaringanExtra::use_clipboard()
xaringanExtra::use_extra_styles(
  hover_code_line = TRUE,         
  mute_unhighlighted_code = TRUE)
```

```{r packages, include=FALSE}
library(RefManageR)
library(kableExtra)
library(tidyverse)
```


```{r, load-refs, include=FALSE, cache=FALSE}
BibOptions(check.entries = FALSE,
           bib.style = "authoryear",
           cite.style = "authoryear",
           style = "markdown",
           hyperlink = FALSE,
           dashed = FALSE,
           longnamesfirst = FALSE,
           max.names = 2)
myBib <- ReadBib("../refs.bib", check = FALSE)
```

<style>
div.blue { background-color:#b0cdef; border-radius: 5px; padding: 20px;}
div.grey { background-color:#d3d3d3; border-radius: 0px; padding: 0px;}
</style>

# Outline

The aim of these slides if to provide an short overview of machine learning and machine learning methods.

---
class: inverse

# What is Machine Learning?


---
# What is Machine Learning?

Machine learning is just statistics for exploring, summarising and modelling data.

--

It can be used for:

* visualisation of high dimensional data  
* discovering patterns/clusters in variables or observations  
* making predictions  
* classifying observations

---
# What is Machine Learning?

In Stage 1, we approached data analysis very much from a hypothesis testing perspective.

--

From this perspective, the goal is to explain a response and determine whether specific variables have a significant effect on the response. 

--

Penalties may be applied for complexity because the interpretability of the model matters.

--

Typically the entire dataset is modelled.

---
# What is Machine Learning?

In machine learning the analysis can be more exploratory.

--

In predictive machine learning the goal is often the quality of the predictions - how predictions arise is less important.

--

Models often have many variables.

--

In predictive machine learning typically 75% of the data are used to train (build) the model with 25% reserved for testing.

---
class: inverse

# Categories of ML methods


---
# Categories of ML methods

ML methods can be characterised as supervised or unsupervised.

--

Supervised methods, like classical statistical learning, are used for prediction and classification. 

--

With supervised learning both the inputs and the outputs are given to the algorithm and the goal is to find the rules to map the inputs to outputs.

--

We train the model on 75% of the data and test it on the remaining 25%.

---
# Categories of ML methods

Unsupervised methods are usually used for exploration, visualisation and data reduction (often needed for visualisation of high dimensional datasets).

--

Unsupervised methods are unsupervised in that they do not use/optimise to a particular output. The goal is to uncover structure.

---
class: inverse

# Supervised methods

---
# Supervised methods

Examples are

Parametric - relatively fast, make assumptions  
* Linear regression - prediction of continuous output  
* Logistic regression - prediction of binary output (classification)  
* Linear Discriminant Analysis - prediction of more than two classes  

Non-parametric - can be slow, can be stochastic   
* Decision trees  
* Naive Bayes
* Support vector machines  
* K nearest neighbour

---
class: inverse

# Supervised methods

---
# Unsupervised methods

Examples are

Parametric - relatively fast, make assumptions  
* Principal Component Analysis    
* Metric Multidimensional scaling  
* many clustering methods  

Non-parametric - can be slow, can be stochastic   
* K means clustering (no relation to K nearest neighbour).  
* Non-Metric Multidimensional scaling
* *t*-distributed stochastic neighbour embedding

---
# References
.font50[
.footnote[
Slides made with xaringan `r Cite(myBib, "xaringan")`,  xaringanExtra `r Cite(myBib, "xaringanExtra")`, xaringanthemer `r Cite(myBib, "xaringanthemer")` and RefManageR `r Cite(myBib, "RefManageR")` 

]

```{r refs, echo=FALSE, results="asis"}
PrintBibliography(myBib)
```

]

---

## Emma Rand <br> [emma.rand@york.ac.uk](mailto:emma.rand@york.ac.uk) <br> Twitter: [@er13_r](https://twitter.com/er13_r) <br> GitHub: [3mmaRand](https://github.com/3mmaRand)  <br> blog: https://buzzrbeeline.blog/
<br>
<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Data Science strand of BIO00058M</span> by <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName">Emma Rand</span> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.
